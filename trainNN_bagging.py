import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import DataLoader, RandomSampler
from nn_Model import nnModel
from evaluation import get_results
import numpy as np
from tqdm import tqdm
import json
import random
import os

def set_seed(seed):
    """è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§"""
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def train_nn(args, train_dataset, device, input_dim, output_dim, go, test_loader, term):
    """
    Bagging è®­ç»ƒå‡½æ•°
    Args:
        train_dataset: PFPDataset å¯¹è±¡ (æ³¨æ„ï¼šè¿™é‡Œæ¥æ”¶çš„æ˜¯æ•°æ®é›†ï¼Œä¸æ˜¯åŠ è½½å™¨)
        test_loader: å›ºå®šçš„æµ‹è¯•é›†åŠ è½½å™¨
    """
    Epoch = 50
    
    # Bagging è®¾ç½®ï¼šä½¿ç”¨5ä¸ªåŸºå­¦ä¹ å™¨
    # è¿™é‡Œçš„ç§å­ç”¨äºæ§åˆ¶ Bootstrap é‡‡æ ·çš„éšæœºæ€§
    seeds = [42, 123, 456, 789, 5959]  
    num_ensembles = len(seeds)
    
    # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ [Num_Models, Num_Samples, Num_Classes]
    all_ensemble_preds = []
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ Starting Bagging Ensemble for term: {term}")
    print(f"   - Base Learners: {num_ensembles}")
    print(f"   - Strategy: Bootstrap Sampling (replacement=True)")
    print(f"{'='*60}\n")

    for ensemble_idx, seed in enumerate(seeds):
        print(f"\n>>> Training Bagging Model {ensemble_idx + 1}/{num_ensembles} (Seed: {seed})")
        
        # 1. è®¾ç½®éšæœºç§å­ (æ§åˆ¶æ¨¡å‹åˆå§‹åŒ– å’Œ æ•°æ®é‡‡æ ·)
        set_seed(seed)
        
        # 2. æ„å»º Bagging æ•°æ®åŠ è½½å™¨ (æ ¸å¿ƒæ­¥éª¤)
        # ä½¿ç”¨ RandomSampler è¿›è¡Œæœ‰æ”¾å›é‡‡æ · (replacement=True)
        # num_samples ä¿æŒä¸åŸæ•°æ®é›†ä¸€è‡´
        bagging_sampler = RandomSampler(
            train_dataset, 
            replacement=True, 
            num_samples=len(train_dataset)
        )
        
        # æ³¨æ„ï¼šä½¿ç”¨äº† sampler åï¼Œshuffle å¿…é¡»ä¸º False
        bagging_loader = DataLoader(
            train_dataset, 
            batch_size=args.batch_size, 
            sampler=bagging_sampler, 
            drop_last=False,
            num_workers=args.num_workers
        )
        
        # 3. åˆå§‹åŒ–æ¨¡å‹
        model = nnModel(output_dim, dropout=args.dropout, device=device, args=args)
        model = model.to(device)
        
        # 4. ä¼˜åŒ–å™¨
        optimizer = optim.Adam(model.parameters(), lr=args.lr/2)
        bceloss = nn.BCELoss()

        # --- Training Loop ---
        for e in range(Epoch):
            model.train()
            epoch_loss = 0
            
            for batch in tqdm(bagging_loader, mininterval=1.0, desc=f'Ep {e+1} Train', leave=False, ncols=80):
                optimizer.zero_grad()
                emb = batch[0].to(device)
                Y_label = batch[1].to(device)
                lm_33 = batch[2].to(device)
                lm_28 = batch[3].to(device)
                lm_23 = batch[4].to(device)

                output, _ = model(emb.squeeze(), lm_33.squeeze(), lm_28.squeeze(), lm_23.squeeze())
                loss_out = bceloss(output, Y_label.squeeze())
                loss_out.backward()
                optimizer.step()
                
                epoch_loss += loss_out.item()

            # ç®€å•æ‰“å°ä¸€ä¸‹è¿›åº¦ (æ¯10ä¸ªepochæˆ–æœ€åä¸€ä¸ª)
            if (e + 1) % 10 == 0 or (e + 1) == Epoch:
                print(f'   Epoch {e+1} | Avg Loss: {epoch_loss / len(bagging_loader):.4f}')

        # 5. æ”¶é›†é¢„æµ‹ç»“æœ (åœ¨å›ºå®šæµ‹è¯•é›†ä¸Š)
        model.eval()
        final_preds = torch.Tensor().to(device)
        final_labels = torch.Tensor().to(device)
        
        with torch.no_grad():
            for batch_test in tqdm(test_loader, mininterval=0.5, desc=f'Inferencing Model {ensemble_idx+1}', leave=False, ncols=80):
                label_test = batch_test[1].to(device)
                emb_test = batch_test[0].to(device)
                lm_33_test = batch_test[2].to(device)
                lm_28_test = batch_test[3].to(device)
                lm_23_test = batch_test[4].to(device)

                output_test, _ = model(emb_test.squeeze(), lm_33_test.squeeze(), lm_28_test.squeeze(), lm_23_test.squeeze())
                final_preds = torch.cat((final_preds, output_test), 0)
                final_labels = torch.cat((final_labels, label_test.squeeze()), 0)
        
        # è¯„ä¼°å•ä¸ªæ¨¡å‹æ€§èƒ½
        current_preds_np = final_preds.cpu().numpy()
        all_ensemble_preds.append(current_preds_np)
        
        perf_single = get_results(go, final_labels.cpu().numpy(), current_preds_np)
        print(f"   âœ… Model {ensemble_idx+1} Result: F-max={perf_single['all']['F-max']:.4f}, M-AUPR={perf_single['all']['M-aupr']:.4f}")

        # ä¿å­˜æ¨¡å‹ (å¯é€‰)
        if args.save_model:
            os.makedirs(f'./data/{args.species}/trained_model/{term}/bagging/', exist_ok=True)
            torch.save(model.state_dict(), f'./data/{args.species}/trained_model/{term}/bagging/model_{ensemble_idx+1}.pkl')
        
        # é‡Šæ”¾æ˜¾å­˜
        del model
        torch.cuda.empty_cache()
    
    # 6. é›†æˆèšåˆ (Averaging)
    print(f"\n{'='*60}")
    print("ğŸ¤ Aggregating Predictions (Bagging)...")
    print(f"{'='*60}")
    
    ensemble_preds = np.mean(all_ensemble_preds, axis=0)
    
    # æœ€ç»ˆè¯„ä¼°
    perf_ensemble = get_results(go, final_labels.cpu().numpy(), ensemble_preds)
    
    print(f"\nğŸ† FINAL BAGGING RESULT [{term.upper()}]:")
    print(f"   M-AUPR: {perf_ensemble['all']['M-aupr']:.6f}")
    print(f"   m-AUPR: {perf_ensemble['all']['m-aupr']:.6f}")
    print(f"   F-max : {perf_ensemble['all']['F-max']:.6f}")
    print(f"{'='*60}\n")
    
    # ä¿å­˜ç»“æœ
    np.save(f'./data/{args.species}/trained_model/{term}/bagging_predictions.npy', ensemble_preds)