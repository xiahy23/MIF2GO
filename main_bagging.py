import argparse
from input_data import load_data, load_labels
from trainAE import train_NoiseGAE
# å¼•å…¥æ–°çš„ Bagging è®­ç»ƒæ¨¡å—
from trainNN_bagging import train_nn 
import numpy as np
import pandas as pd
import os
import torch
from preprocessing import PFPDataset
from torch.utils.data import DataLoader
import warnings
import shutil
import time

def reshape(features):
    return np.hstack(features).reshape((len(features),len(features[0])))

def train(args):
    # 1. è‡ªåŠ¨æ›¿æ¢ nn_Model æ–‡ä»¶ (åŸºäºä½ ä¹‹å‰çš„æ–¹æ¡ˆï¼Œç¡®ä¿ç”¨çš„æ˜¯è½»é‡åŒ–æ¨¡å‹)
    # å»ºè®®ä½¿ç”¨ v2 (Attention Pooling) é…åˆ Baggingï¼Œæ•ˆæœæœ€ç¨³
    source_file = f'nn_Model_{args.model_select}.py'
    target_file = 'nn_Model.py'
    
    if os.path.exists(source_file):
        print(f"ğŸ”„ [Auto-Switch] Overwriting nn_Model.py with {source_file} ...")
        shutil.copy(source_file, target_file)
        time.sleep(1) 
    else:
        print(f"âš ï¸ Warning: {source_file} not found. Using current nn_Model.py")

    # å¿…é¡»åœ¨æ›¿æ¢æ–‡ä»¶åå¼•å…¥ nn_Modelï¼Œè™½ç„¶ train_nn å†…éƒ¨ä¹Ÿä¼šå¼•
    # è¿™é‡Œä¸»è¦æ˜¯ä¸ºäº†é˜²æ­¢ä¸»è¿›ç¨‹ç¼“å­˜
    import importlib
    import trainNN_bagging
    importlib.reload(trainNN_bagging)

    print("loading features...")
    uniprot = pd.read_pickle(os.path.join(args.data_path, args.species, "features.pkl"))

    device = torch.device('cuda:'+args.device)
    
    # åŠ è½½å›¾åµŒå…¥
    if 'embeddings.npy' not in os.listdir('./data/'+args.species+'/trained_emb_files/'):
        for graph in args.graphs:
            print(f"Processing {graph} data...")
            if graph == 'ppi':
                ppi_adj, ppi_features = load_data(graph, uniprot, args)
            else:
                ssn_adj, ssn_features = load_data(graph, uniprot, args)
        embeddings = train_NoiseGAE(ppi_features, ppi_adj,ssn_features,ssn_adj, args,device)
    else:
        print("Loading cached embeddings...")
        embeddings = np.load('./data/'+args.species+'/trained_emb_files/embeddings.npy')

    np.random.seed(5959)
    cc, mf, bp = load_labels(uniprot)

    # æ•°æ®åˆ’åˆ†
    num_test = int(np.floor(cc.shape[0] / 5.))
    num_train = cc.shape[0] - num_test

    if 'data_idx.txt' not in os.listdir('./data/'+args.species):
        all_idx = list(range(cc.shape[0]))
        np.random.shuffle(all_idx)
        with open('./data/'+args.species+'/data_idx.txt','a') as f:
            for idx in all_idx:
                f.write(str(idx)+'\n')
    else:
        all_idx = []
        with open('./data/'+args.species+'/data_idx.txt') as f:
            for line in f:
                all_idx.append(int(line.strip()))

    train_idx = all_idx[:num_train]
    test_idx = all_idx[num_train:(num_train + num_test)]

    # åŠ è½½ ESM ç‰¹å¾
    ESM_33 = np.load('./data/'+args.species+'/ESM-2_33.npy')
    ESM_28 = np.load('./data/' + args.species + '/ESM-2_28.npy')
    ESM_23 = np.load('./data/' + args.species + '/ESM-2_23.npy')

    Y_train_cc = cc[train_idx]; Y_test_cc = cc[test_idx]
    Y_train_bp = bp[train_idx]; Y_test_bp = bp[test_idx]
    Y_train_mf = mf[train_idx]; Y_test_mf = mf[test_idx]

    X_train = embeddings[train_idx]
    X_test = embeddings[test_idx]

    LM_train = [ESM_33[train_idx], ESM_28[train_idx], ESM_23[train_idx]]
    LM_test = [ESM_33[test_idx], ESM_28[test_idx], ESM_23[test_idx]]

    # 2. åˆ›å»ºæ•°æ®é›† (Dataset Objects)
    # æ³¨æ„ï¼šè¿™é‡Œä¸åˆ›å»º DataLoaderï¼Œå› ä¸º Bagging éœ€è¦åœ¨å†…éƒ¨è¿›è¡Œéšæœºé‡‡æ ·
    train_data_cc = PFPDataset(emb_X=X_train, data_Y=Y_train_cc, args=args, global_lm=LM_train)
    train_data_bp = PFPDataset(emb_X=X_train, data_Y=Y_train_bp, args=args, global_lm=LM_train)
    train_data_mf = PFPDataset(emb_X=X_train, data_Y=Y_train_mf, args=args, global_lm=LM_train)

    test_data_cc = PFPDataset(emb_X=X_test, data_Y=Y_test_cc, args=args, global_lm=LM_test)
    test_data_bp = PFPDataset(emb_X=X_test, data_Y=Y_test_bp, args=args, global_lm=LM_test)
    test_data_mf = PFPDataset(emb_X=X_test, data_Y=Y_test_mf, args=args, global_lm=LM_test)

    # æµ‹è¯•é›†çš„ Loader æ˜¯å›ºå®šçš„ï¼Œä¸éœ€è¦ Bagging
    dataset_test_cc = DataLoader(test_data_cc, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=args.num_workers)
    dataset_test_bp = DataLoader(test_data_bp, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=args.num_workers)
    dataset_test_mf = DataLoader(test_data_mf, batch_size=args.batch_size, shuffle=False, drop_last=False, num_workers=args.num_workers)

    print("\n" + "="*60)
    print(f"ğŸš€ Running Bagging Ensemble (Model: {args.model_select})")
    print("="*60)

    # 3. è®­ç»ƒ MF ä»»åŠ¡
    print("\n>>> Processing MF Task...")
    trainNN_bagging.train_nn(
        args=args, device=device, 
        input_dim=embeddings.shape[1], output_dim=Y_train_mf.shape[1],
        train_dataset=train_data_mf, # ä¼ å…¥ Dataset
        test_loader=dataset_test_mf, # ä¼ å…¥ Loader
        go=mf, term='mf'
    )

    # 4. è®­ç»ƒ BP ä»»åŠ¡
    print("\n>>> Processing BP Task...")
    trainNN_bagging.train_nn(
        args=args, device=device, 
        input_dim=embeddings.shape[1], output_dim=Y_train_bp.shape[1],
        train_dataset=train_data_bp, 
        test_loader=dataset_test_bp, 
        go=bp, term='bp'
    )

    # 5. è®­ç»ƒ CC ä»»åŠ¡
    print("\n>>> Processing CC Task...")
    trainNN_bagging.train_nn(
        args=args, device=device, 
        input_dim=embeddings.shape[1], output_dim=Y_train_cc.shape[1],
        train_dataset=train_data_cc, 
        test_loader=dataset_test_cc, 
        go=cc, term='cc'
    )

if __name__ == "__main__":
    os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    warnings.filterwarnings("ignore")
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    # å…³é”®å‚æ•°
    parser.add_argument('--model_select', type=str, default='v2', 
                       help="é€‰æ‹©åŸºæ¨¡å‹æ¶æ„: 'v2' (Attention Pooling - æ¨è), 'v3' (Gated), 'original'")
    
    parser.add_argument('--ppi_attributes', type=int, default=5)
    parser.add_argument('--simi_attributes', type=int, default=5)
    parser.add_argument('--graphs', type=lambda s: [item for item in s.split(",")], default=['ppi','sequence_similarity'])
    parser.add_argument('--species', type=str, default="Human")
    parser.add_argument('--data_path', type=str, default="./data/")
    
    # Dropout å»ºè®®è®¾å¤§ä¸€ç‚¹é…åˆ Bagging
    parser.add_argument('--dropout', type=float, default=0.5, help="Dropout rate.") 
    
    parser.add_argument('--hidden1', type=int, default=800)
    parser.add_argument('--hidden2', type=int, default=400)
    parser.add_argument('--lr', type=float, default=0.001)
    parser.add_argument('--epochs', type=int, default=160)
    parser.add_argument('--device', type=str, default='0')
    parser.add_argument('--thr_combined', type=float, default=0.4)
    parser.add_argument('--thr_evalue', type=float, default=1e-4)
    parser.add_argument('--noise_rate', type=float, default=0.6)
    parser.add_argument('--alpha', type=int, default=2)
    parser.add_argument('--eps', type=float, default=2.0)
    parser.add_argument('--heads', type=int, default=4)
    parser.add_argument('--lambda_', type=float, default=0.4)
    parser.add_argument('--num_workers', type=int, default=8)
    parser.add_argument('--batch_size', type=int, default=128)
    parser.add_argument('--save_model', type=bool, default=False)

    args = parser.parse_args()
    train(args)